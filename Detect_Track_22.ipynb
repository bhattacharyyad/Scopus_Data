{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharyyad/Scopus_Data/blob/master/Detect_Track_22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l3M85I88jX0"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow # Import cv2_imshow for Colab display\n",
        "import time\n",
        "\n",
        "# Load the YOLO model\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# Label dictionary\n",
        "label_dict = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign',\n",
        "         12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
        "\n",
        "# --- Removed tkinter GUI setup ---\n",
        "# Interactive elements like label selection and confidence slider are removed.\n",
        "# Default values for these will be used for demonstration.\n",
        "selected_labels = ['person', 'car', 'bicycle'] # Example: default selected labels\n",
        "conf_threshold = 0.5 # Example: default confidence threshold\n",
        "\n",
        "# List to store polygon points (will not be interactive without GUI)\n",
        "# For demonstration, we can define a static polygon if needed, or remove this feature.\n",
        "# For now, it will be kept but not interactively modifiable.\n",
        "polygon_points = []\n",
        "\n",
        "def process_video():\n",
        "    # Define maximum width and height\n",
        "    max_width = 600\n",
        "    max_height = 400\n",
        "\n",
        "    # Open video capture\n",
        "    # Make sure 'room-video.mp4' is available in your Colab environment\n",
        "    cap = cv2.VideoCapture(\"room-video2.mp4\")\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video file.\")\n",
        "        return\n",
        "\n",
        "    # Get video properties for saving (optional, but good practice)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for .mp4\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    out_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    out_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    out = cv2.VideoWriter('output.mp4', fourcc, fps, (out_width, out_height))\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Resize frame if it exceeds maximum width or height\n",
        "        height, width = frame.shape[:2]\n",
        "        if width > max_width or height > max_height:\n",
        "            # Calculate scaling factor\n",
        "            scale = min(max_width / width, max_height / height)\n",
        "            new_width = int(width * scale)\n",
        "            new_height = int(height * scale)\n",
        "            frame = cv2.resize(frame, (new_width, new_height))\n",
        "\n",
        "        # Run YOLO detection\n",
        "        results = model(frame)\n",
        "\n",
        "        # Convert polygon points to OpenCV coordinates (if a static polygon is defined)\n",
        "        polygon_pts = None\n",
        "        if polygon_points: # If you manually define polygon_points for testing\n",
        "            polygon_pts = np.array([(x, y) for x, y in polygon_points], np.int32)\n",
        "            polygon_pts = polygon_pts.reshape((-1, 1, 2))\n",
        "\n",
        "        # Filter results based on selected labels and confidence score\n",
        "        for result in results:\n",
        "            for id, box in enumerate(result.boxes.xyxy):\n",
        "                class_id = int(result.boxes.cls[id])\n",
        "                label = label_dict[class_id]\n",
        "                conf = result.boxes.conf[id]\n",
        "\n",
        "                if label in selected_labels and conf >= conf_threshold:\n",
        "                    x1, y1, x2, y2 = map(int, box)\n",
        "                    center = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
        "\n",
        "                    # Check if center is inside the polygon (if defined)\n",
        "                    if polygon_pts is not None:\n",
        "                        distance = cv2.pointPolygonTest(polygon_pts, center, False)\n",
        "                        if distance >= 0:\n",
        "                            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
        "                            cv2.putText(frame, label, (x2, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
        "                    else:\n",
        "                         cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
        "                         cv2.putText(frame, label, (x2, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
        "\n",
        "        # Draw the polygon on the frame (if defined)\n",
        "        if polygon_pts is not None and len(polygon_points) > 1:\n",
        "            cv2.polylines(frame, [polygon_pts], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "\n",
        "        # Display the frame in Colab\n",
        "        cv2_imshow(frame)\n",
        "        out.write(frame) # Write the frame to the output video\n",
        "\n",
        "        # Introduce a small delay to control playback speed and allow display updates\n",
        "        # cv2.waitKey(1) or time.sleep() cannot be used directly to control cv2_imshow effectively in a loop.\n",
        "        # The display will update with each call to cv2_imshow.\n",
        "\n",
        "        # To stop the loop, you might need to manually interrupt the cell execution.\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    # cv2.destroyAllWindows() # Not necessary for Colab's cv2_imshow\n",
        "    print(\"Video processing finished. Output saved to output.mp4\")\n",
        "\n",
        "# --- Removed tkinter event bindings and mainloop ---\n",
        "# The video processing will run once when called.\n",
        "process_video()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "id": "LlXFzN7zjuZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f40c820"
      },
      "outputs": [],
      "source": [
        "!ls /content/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLO from ultralytics (object detection & tracking)\n",
        "!pip install ultralytics        # https://pypi.org/project/ultralytics/\n",
        "\n",
        "# OpenCV for video/image processing\n",
        "!pip install opencv-python      # https://pypi.org/project/opencv-python/\n",
        "\n",
        "# PyTorch CPU version (works without GPU)\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "# If you have a GPU and want GPU acceleration, uncomment the next line:\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
      ],
      "metadata": {
        "id": "SQi2YGCWc4bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import time\n",
        "import torch\n",
        "import subprocess\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "print(torch.__version__) # my version --> 2.2.0+cu121\n",
        "print(torch.cuda.is_available()) # True (GPU is available)\n"
      ],
      "metadata": {
        "id": "JNTFm2gpdIWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov8n.pt')"
      ],
      "metadata": {
        "id": "zkMdQ-AVdtgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(tracker_choice='bytetrack'):\n",
        "    input_path = 'flight1.mp4'  # Your video file path\n",
        "\n",
        "    # Validate tracker choice\n",
        "    if tracker_choice not in ['botsort', 'bytetrack']:\n",
        "        print(f\"Invalid tracker choice '{tracker_choice}', defaulting to 'bytetrack'\")\n",
        "        tracker_choice = 'bytetrack'\n",
        "\n",
        "    # Load the pretrained YOLOv8 model\n",
        "    model = YOLO('yolov8n.pt')\n",
        "\n",
        "    # Check if CUDA is available and set device accordingly\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model.to(device)\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load the video file\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error opening video file {input_path}\")\n",
        "        return\n",
        "\n",
        "    # Initialize variables for FPS calculation\n",
        "    prev_time = 0\n",
        "\n",
        "    \"\"\"\n",
        "    Parameters of track method:\n",
        "    source: str - path to video file or camera index\n",
        "    tracker: str - tracker configuration file (e.g., 'bytetrack.yaml' or 'botsort.yaml')\n",
        "    conf: float - confidence threshold for detections\n",
        "    stream: bool - if True, yields frames one by one for real-time processing,\n",
        "                if False, processes the entire video at once\n",
        "    \"\"\"\n",
        "    results = model.track(\n",
        "        source=input_path, # path to video file\n",
        "        tracker=f'{tracker_choice}.yaml',  # 'bytetrack.yaml' or 'botsort.yaml'\n",
        "        conf=0.3,   # confidence threshold\n",
        "        stream=True # set it to True for continuous video processing\n",
        "    )\n",
        "\n",
        "    # loop through the results\n",
        "    for frame_result in results:\n",
        "        # Get the original frame\n",
        "        img = frame_result.orig_img.copy()\n",
        "\n",
        "        # Calculate FPS (fall back to input_fps until prev_time set)\n",
        "        curr_time = time.time()\n",
        "        fps = 1 / (curr_time - prev_time) if prev_time != 0 else cap.get(cv2.CAP_PROP_FPS)\n",
        "        prev_time = curr_time\n",
        "\n",
        "        # loop through the detected boxes and draw them on the frame\n",
        "        for box in frame_result.boxes:\n",
        "            # Extract bounding box coordinates\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy.cpu().numpy()[0])\n",
        "            # Extract confidence\n",
        "            conf = box.conf.cpu().item()\n",
        "            # Extract class\n",
        "            cls = int(box.cls.cpu().item())\n",
        "            # Extract track ID\n",
        "            track_id = int(box.id.cpu().item()) if box.id is not None else -1\n",
        "\n",
        "            \"\"\"\n",
        "            model.names is a dictionary mapping class indices to class names.\n",
        "            {0: 'person',\n",
        "            1: 'bicycle',\n",
        "            2: 'car',\n",
        "            3: 'motorcycle',\n",
        "            ...,\n",
        "            }\n",
        "            \"\"\"\n",
        "            class_name = model.names[cls]\n",
        "\n",
        "            # Draw bounding box and label\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            label = f\"{class_name} ID:{track_id} {conf:.2f}\"\n",
        "            cv2.putText(img, label, (x1, y1 - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        # Display tracker name and FPS on top-left corner\n",
        "        cv2.putText(img, f\"Tracker: {tracker_choice}\", (10, 70),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 3)\n",
        "        cv2.putText(img, f\"FPS: {fps:.2f}\", (10, 150),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 3)\n",
        "\n",
        "        # Display the frame with detections\n",
        "        from google.colab.patches import cv2_imshow # Import cv2_imshow for Colab display\n",
        "        cv2_imshow(img)\n",
        "\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    # cv2.destroyAllWindows() # Not necessary for Colab's cv2_imshow\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \"\"\" There are two trackers available:\n",
        "        1. ByteTrack (bytetrack.yaml)\n",
        "        2. Sort (botsort.yaml)\n",
        "    \"\"\"\n",
        "    main(tracker_choice='bytetrack')"
      ],
      "metadata": {
        "id": "3Y2pQQ5Md0Tz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6qV1H+B42pDB1Wk4egTj9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}